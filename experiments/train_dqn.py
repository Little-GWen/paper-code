import sys, os, argparse

# --- 1. è·¯å¾„ä¿®æ­£ ---
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from multiprocessing import Manager
import gymnasium as gym
import torch
import numpy as np

from models.agent_dqn import Agent_DQN
from config.config import *

# --- 2. [å…³é”®ä¿®å¤] å¿…é¡»å¯¼å…¥è‡ªå®šä¹‰ç¯å¢ƒä»¥è§¦å‘æ³¨å†Œ ---
import custom_merge_env


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--seed', type=int, default=0)
    return parser.parse_args()


def main_optimizer(env_id, num_episodes, agent, save_dir):
    if not os.path.exists(save_dir): os.makedirs(save_dir)
    weights_path = os.path.join(save_dir, 'weights.pth')

    # åˆ›å»ºç¯å¢ƒ
    env = gym.make(env_id)

    # æ—¥å¿—åˆ—è¡¨
    rewards_log = []
    speed_log = []
    collision_log = []

    print(f"ğŸš€ DQN Training Started! Episodes: {num_episodes}")
    print(f"   Device: {agent.device}")
    print(f"   Batch Size: {agent.bs}")

    for i in range(num_episodes):
        state, _ = env.reset()  # Gym æ–°ç‰ˆ reset è¿”å› (obs, info)
        state = state.flatten()

        ep_reward, steps, done = 0, 0, False
        ep_speed = 0
        is_crashed = 0

        while not done and steps < MAX_T:
            action, _ = agent.act(state)  # DQN act è¿”å› (action, 0.0)

            res = env.step(action)
            # å…¼å®¹ 4-tuple æˆ– 5-tuple è¿”å›
            if len(res) == 5:
                next_state, reward, term, trunc, info = res
                done = term or trunc
            else:
                next_state, reward, done, info = res

            if len(state) >= 3: ep_speed += state[2]  # è®°å½•é€Ÿåº¦

            # è®°å½•ç¢°æ’çŠ¶æ€
            if done:
                is_crashed = info.get('crashed', False) or getattr(env.unwrapped.vehicle, 'crashed', False)

            next_state = next_state.flatten()

            # å­˜å…¥ç»éªŒå›æ”¾ (log_prob å ä½ç¬¦å¡« 0)
            agent.memory.remember((state, action, reward, next_state, done, 0))

            # å­¦ä¹ æ­¥éª¤
            if steps % 10 == 0:
                agent.learn()

            state = next_state
            ep_reward += reward
            steps += 1

        # è®°å½•æ—¥å¿—
        rewards_log.append(ep_reward)
        speed_log.append(ep_speed / max(1, steps))
        collision_log.append(1 if is_crashed else 0)

        # æ‰“å°è¿›åº¦
        if i % 10 == 0:
            print(
                f"\rEp {i}/{num_episodes} | Rew: {ep_reward:.1f} | Eps: {agent.epsilon:.2f} | Mem: {len(agent.memory)}",
                end='')

        # ä¿å­˜æ¨¡å‹å’Œæ•°æ®
        if i % 50 == 0:
            torch.save(agent.q_net.state_dict(), weights_path)
            np.save(os.path.join(save_dir, 'rewards.npy'), rewards_log)
            np.save(os.path.join(save_dir, 'speed.npy'), speed_log)
            np.save(os.path.join(save_dir, 'collision.npy'), collision_log)

    # è®­ç»ƒç»“æŸä¿å­˜
    torch.save(agent.q_net.state_dict(), weights_path)
    np.save(os.path.join(save_dir, 'rewards.npy'), rewards_log)
    np.save(os.path.join(save_dir, 'speed.npy'), speed_log)
    np.save(os.path.join(save_dir, 'collision.npy'), collision_log)
    print(f"\nDQN Finished! Saved to {save_dir}")


if __name__ == '__main__':
    args = parse_args()
    set_seed(args.seed)
    save_dir = f'results/dqn/seed_{args.seed}'

    # ä½¿ç”¨ Manager åªæ˜¯ä¸ºäº†å…¼å®¹ ReplayBuffer æ¥å£ï¼ŒDQN æœ¬èº«æ˜¯å•è¿›ç¨‹çš„
    with Manager() as manager:
        # åˆ›å»º Dummy ç¯å¢ƒè·å–ç»´åº¦
        dummy = gym.make(RAM_ENV_NAME)
        state_dim = int(np.prod(dummy.observation_space.shape))
        act_dim = dummy.action_space.n
        dummy.close()

        # --- 3. [å…³é”®è°ƒæ•´] è¶…å‚æ•°ä¼˜åŒ– ---
        # åŸæ¥æ˜¯ 64ï¼Œå»ºè®®æ”¹æˆ 256 æˆ– 512ï¼Œå¦åˆ™é«˜å¯†åº¦ä¸‹å­¦ä¸åŠ¨
        BATCH_SIZE_DQN = 256

        # å¢åŠ æ€»å›åˆæ•°ï¼ŒåŸæ¥ //4 å¯èƒ½å¤ªå°‘äº†ï¼Œè·‘ 10000 è½®çœ‹çœ‹
        TOTAL_EPISODES = 80000

        agent = Agent_DQN(
            state_dim,
            act_dim,
            bs=BATCH_SIZE_DQN,
            lr=5e-4,
            gamma=0.99,
            epsilon_start=1.0,
            epsilon_end=0.05,
            epsilon_decay=0.9995,  # è¡°å‡æ…¢ä¸€ç‚¹ï¼Œå¤šæ¢ç´¢ä¸€ä¼šå„¿
            device=DEVICE,
            manager=manager
        )

        main_optimizer(RAM_ENV_NAME, TOTAL_EPISODES, agent, save_dir)